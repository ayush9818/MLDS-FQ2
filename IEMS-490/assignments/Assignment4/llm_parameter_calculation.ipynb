{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameters(config, use_mqa=True):\n",
    "    \"\"\"\n",
    "    Calculate the total number of parameters for an LLM architecture.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Dictionary containing the model's configuration parameters.\n",
    "        use_mqa (bool): Flag indicating whether to consider Multi-Query Attention.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of parameters in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract necessary parameters from the configuration\n",
    "    vocab_size = config.get('vocab_size')\n",
    "    hidden_size = config.get('hidden_size')\n",
    "    intermediate_size = config.get('intermediate_size')\n",
    "    num_hidden_layers = config.get('num_hidden_layers')\n",
    "    num_attention_heads = config.get('num_attention_heads')\n",
    "    num_key_value_heads = config.get('num_key_value_heads', num_attention_heads)\n",
    "    head_dim = config.get('head_dim')\n",
    "    mlp_bias = config.get('mlp_bias', True)\n",
    "    attention_bias = config.get('attention_bias', True)\n",
    "    tie_word_embeddings = config.get('tie_word_embeddings', True)\n",
    "\n",
    "    if head_dim is None:\n",
    "        head_dim = hidden_size // num_attention_heads\n",
    "\n",
    "    # Initialize total parameters\n",
    "    total_params = 0\n",
    "\n",
    "    # 1. Embedding Layer\n",
    "    # Token embeddings: vocab_size x hidden_size\n",
    "    embedding_params = vocab_size * hidden_size\n",
    "    total_params += embedding_params\n",
    "\n",
    "    # 2. Transformer Layers\n",
    "    transformer_layer_params = 0\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        layer_params = 0\n",
    "\n",
    "        # a. Layer Normalization (RMSNorm)\n",
    "        # Assuming scale parameters only (no bias)\n",
    "        layernorm_params = 2 * hidden_size  # Two LayerNorms per layer\n",
    "        layer_params += layernorm_params\n",
    "\n",
    "        # b. Self-Attention Mechanism\n",
    "        if use_mqa:\n",
    "            # Multi-Query Attention\n",
    "            # Query projection (W_q): hidden_size x hidden_size\n",
    "            W_q_params = hidden_size * hidden_size\n",
    "            # Key and Value projections (W_kv): hidden_size x (2 * head_dim * num_key_value_heads)\n",
    "            W_kv_params = hidden_size * (2 * head_dim * num_key_value_heads)\n",
    "            # Output projection (W_o): hidden_size x hidden_size\n",
    "            W_o_params = hidden_size * hidden_size\n",
    "            # Biases (if any)\n",
    "            attention_bias_params = 0\n",
    "            if attention_bias:\n",
    "                # Biases for W_q, W_kv, and W_o\n",
    "                attention_bias_params = hidden_size + 2 * head_dim * num_key_value_heads + hidden_size\n",
    "            attention_params = W_q_params + W_kv_params + W_o_params + attention_bias_params\n",
    "        else:\n",
    "            # Standard Multi-Head Attention\n",
    "            # Query, Key, Value projections (W_q, W_k, W_v): each hidden_size x hidden_size\n",
    "            W_q_params = hidden_size * hidden_size\n",
    "            W_k_params = hidden_size * hidden_size\n",
    "            W_v_params = hidden_size * hidden_size\n",
    "            # Output projection (W_o): hidden_size x hidden_size\n",
    "            W_o_params = hidden_size * hidden_size\n",
    "            # Biases (if any)\n",
    "            attention_bias_params = 0\n",
    "            if attention_bias:\n",
    "                # Biases for W_q, W_k, W_v, and W_o\n",
    "                attention_bias_params = 3 * hidden_size + hidden_size\n",
    "            attention_params = W_q_params + W_k_params + W_v_params + W_o_params + attention_bias_params\n",
    "\n",
    "        layer_params += attention_params\n",
    "\n",
    "        # c. Feed-Forward Network (MLP)\n",
    "        gate_proj = hidden_size * intermediate_size\n",
    "        up_proj = hidden_size * intermediate_size\n",
    "        down_proj = intermediate_size * hidden_size\n",
    "        # Biases (if any)\n",
    "        mlp_bias_params = 0\n",
    "        if mlp_bias:\n",
    "            mlp_bias_params = intermediate_size + hidden_size\n",
    "        mlp_params = gate_proj + up_proj + down_proj + mlp_bias_params\n",
    "\n",
    "        layer_params += mlp_params\n",
    "\n",
    "        # Add layer parameters to total transformer parameters\n",
    "        transformer_layer_params += layer_params\n",
    "\n",
    "    total_params += transformer_layer_params\n",
    "\n",
    "    # 3. Final Layer Normalization\n",
    "    # Assuming scale parameters only (no bias)\n",
    "    final_layernorm_params = hidden_size\n",
    "    total_params += final_layernorm_params\n",
    "\n",
    "    # 4. Output Projection Layer (if embeddings are not tied)\n",
    "    if not tie_word_embeddings:\n",
    "        output_projection_params = hidden_size * vocab_size\n",
    "        total_params += output_projection_params\n",
    "\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA-3.2-1B\n",
    "\n",
    "**Config**\n",
    "\n",
    "-  head_dim : 64\n",
    "- hidden_size : 2048\n",
    "- intermediate_size : 8192\n",
    "- mlp_bias : false\n",
    "- num_attention_heads : 32\n",
    "- num_hidden_layers : 16\n",
    "- num_key_value_heads : 8\n",
    "- vocab_size : 128256\n",
    "- tie_word_embeddings : true\n",
    "\n",
    "\n",
    "## Parameter Calculation\n",
    "\n",
    "### **1. Embedding Layer**\n",
    "\n",
    "- **Token Embeddings**: Maps `vocab_size` tokens to `hidden_size` embeddings.\n",
    "  - Parameters: `vocab_size × hidden_size`\n",
    "  - Calculation: `128,256 × 2,048 = 262,668,288`\n",
    "\n",
    "### **2. Transformer Layers**\n",
    "\n",
    "There are `num_hidden_layers = 16` Transformer layers. Each layer consists of:\n",
    "\n",
    "#### **a. Layer Normalization (RMSNorm)**\n",
    "\n",
    "- **Parameters per layer**: `2 × hidden_size` (since there are two RMSNorms per layer)\n",
    "  - Calculation: `2 × 2,048 = 4,096`\n",
    "\n",
    "#### **b. Self-Attention Mechanism**\n",
    "\n",
    "- **Query Projection (W_q)**:\n",
    "  - Parameters: `hidden_size × (num_attention_heads * head_dim)`\n",
    "  - Calculation: `2,048 × (64x32) = 4,194,304`\n",
    "  \n",
    "- **Key and Value Projections (W_kv)**:\n",
    "  - Parameters: `hidden_size × (2 × head_dim × num_key_value_heads)`\n",
    "  - Calculation: `2,048 × (2 × 64 × 8) = 2,097,152`\n",
    "  \n",
    "- **Output Projection (W_o)**:\n",
    "  - Parameters: `hidden_size × hidden_size`\n",
    "  - Calculation: `2,048 × 2,048 = 4,194,304`\n",
    "  \n",
    "- **Total Attention Parameters per layer**:\n",
    "  - Calculation: `4,194,304 (W_q) + 2,097,152 (W_kv) + 4,194,304 (W_o) = 10,485,760`\n",
    "\n",
    "#### **c. Feed-Forward Network (MLP)**\n",
    "\n",
    "- **up_proj**:\n",
    "  - Parameters: `hidden_size × intermediate_size`\n",
    "  - Calculation: `2,048 × 8,192 = 16,777,216`\n",
    "\n",
    "- **gate_proj**:\n",
    "  - Parameters: `hidden_size x intermediate_size`\n",
    "  - Calculation: `2048 x 8,192 = 16,777,216`\n",
    "\n",
    "\n",
    "- **down_proj**:\n",
    "  - Parameters: `intermediate_size × hidden_size`\n",
    "  - Calculation: `8,192 × 2,048 = 16,777,216`\n",
    "\n",
    "\n",
    "  \n",
    "- **Total MLP Parameters per layer**:\n",
    "  - Calculation: `16,777,216 (up_proj) + 16,777,216 (down_proj) + 16,777,216 (gate_proj) = 50,331,648`\n",
    "\n",
    "#### **d. Total Parameters per Transformer Layer**\n",
    "\n",
    "- **Sum of all components per layer**:\n",
    "  - Calculation: `4,096 (LayerNorm) + 10,485,760 (Attention) + 50,331,648 (MLP) = 60,821,504`\n",
    "\n",
    "#### **e. Total Parameters for All Transformer Layers**\n",
    "\n",
    "- **Total for 16 layers**:\n",
    "  - Calculation: `60,821,504 × 16 = 973,144,064`\n",
    "\n",
    "### **3. Final Layer Normalization**\n",
    "\n",
    "- **Parameters**: `hidden_size`\n",
    "  - Calculation: `2,048`\n",
    "\n",
    "### **4. Total Model Parameters**\n",
    "\n",
    "- **Sum of Embedding Layer, Transformer Layers, and Final LayerNorm**:\n",
    "  - Calculation: `262,668,288 (Embeddings) + 973,144,064 (Transformer Layers) + 2,048 (Final LayerNorm) = 1,235,814,400`\n",
    "\n",
    "Therefore, the LLM has a total of approximately 1B parameters. Here I have assumed the output softmax parameters are shared by input embedding layer parameters. This is assuming multi query attention also, so we achieve reduction of few parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters with MQA: 1235814400\n",
      "Number of parameters without MQA: 1336477696\n"
     ]
    }
   ],
   "source": [
    "## Code Verification\n",
    "\n",
    "config_llama3_2_1B = json.load(open('configs/llama-3.2-1B.json'))\n",
    "\n",
    "num_params_with_mqa = calculate_parameters(config_llama3_2_1B, use_mqa=True)\n",
    "num_params_without_mqa = calculate_parameters(config_llama3_2_1B, use_mqa=False)\n",
    "print(f\"Number of parameters with MQA: {num_params_with_mqa}\")\n",
    "print(f\"Number of parameters without MQA: {num_params_without_mqa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA-3.2-3B\n",
    "\n",
    "**Config**\n",
    "\n",
    "- head_dim : 128\n",
    "- hidden_size : 3072\n",
    "- intermediate_size : 8192\n",
    "- mlp_bias : false\n",
    "- num_attention_heads : 24\n",
    "- num_hidden_layers : 28\n",
    "- num_key_value_heads : 8\n",
    "- vocab_size : 128256\n",
    "- tie_word_embeddings : true\n",
    "\n",
    "### **1. Embedding Layer**\n",
    "\n",
    "- **Token Embeddings**: Maps `vocab_size` tokens to `hidden_size` embeddings.\n",
    "  - **Parameters**: `vocab_size × hidden_size`\n",
    "  - **Calculation**: `128,256 × 3,072 = 394,002,432`\n",
    "\n",
    "### **2. Transformer Layers**\n",
    "\n",
    "There are `num_hidden_layers = 28` Transformer layers. Each layer consists of:\n",
    "\n",
    "#### **a. Layer Normalization (RMSNorm)**\n",
    "\n",
    "- **Parameters per layer**: `2 × hidden_size` (since there are two RMSNorms per layer)\n",
    "  - **Calculation**: `2 × 3,072 = 6,144`\n",
    "\n",
    "#### **b. Self-Attention Mechanism**\n",
    "\n",
    "- **Query Projection (W_q)**:\n",
    "  - **Parameters**: `hidden_size × hidden_size`\n",
    "  - **Calculation**: `3,072 × 3,072 = 9,437,184`\n",
    "  \n",
    "- **Key and Value Projections (W_kv)**:\n",
    "  - **Parameters**: `hidden_size × (2 × head_dim × num_key_value_heads)`\n",
    "  - **Calculation**: `3,072 × (2 × 128 × 8) = 3,072 × 2,048 = 6,291,456`\n",
    "  \n",
    "- **Output Projection (W_o)**:\n",
    "  - **Parameters**: `hidden_size × hidden_size`\n",
    "  - **Calculation**: `3,072 × 3,072 = 9,437,184`\n",
    "  \n",
    "- **Total Attention Parameters per layer**:\n",
    "  - **Calculation**: `9,437,184 (W_q) + 6,291,456 (W_kv) + 9,437,184 (W_o) = 25,165,824`\n",
    "\n",
    "#### **c. Feed-Forward Network (MLP)**\n",
    "\n",
    "- **Gate Proj**:\n",
    "  - **Parameters**: `hidden_size × intermediate_size`\n",
    "  - **Calculation**: `3,072 × 8,192 = 25,165,824`\n",
    "\n",
    "- **Up Proj**:\n",
    "  - **Parameters**: `hidden_size × intermediate_size`\n",
    "  - **Calculation**: `3,072 × 8,192 = 25,165,824`\n",
    "\n",
    "- **Down Proj**:\n",
    "  - **Parameters**: `intermediate_size * hidden_size`\n",
    "  - **Calculation**: `8,192 x 3,072 = 25,165,824`\n",
    "  \n",
    "- **Total MLP Parameters per layer**:\n",
    "  - **Calculation**: `25,165,824 (up_proj) + 25,165,824 (down_proj) + 25,165,824 (up_proj) = 75,497,472`\n",
    "\n",
    "#### **d. Total Parameters per Transformer Layer**\n",
    "\n",
    "- **Sum of all components per layer**:\n",
    "  - **Calculation**: `6,144 (LayerNorm) + 25,165,824 (Attention) + 75,497,472 (MLP) = 100,669,440`\n",
    "\n",
    "#### **e. Total Parameters for All Transformer Layers**\n",
    "\n",
    "- **Total for 28 layers**:\n",
    "  - **Calculation**: `100,669,440 × 28 = 2,818,744,320`\n",
    "\n",
    "### **3. Final Layer Normalization**\n",
    "\n",
    "- **Parameters**: `hidden_size`\n",
    "  - **Calculation**: `3,072`\n",
    "\n",
    "### **4. Total Model Parameters**\n",
    "\n",
    "- **Sum of Embedding Layer, Transformer Layers, and Final LayerNorm**:\n",
    "  - **Calculation**: `394,002,432 (Embeddings) + 2,818,744,320 (Transformer Layers) + 3,072 (Final LayerNorm) = 3,212,749,824`\n",
    "\n",
    "**Answer**: The model has a total of 3B parameters.  Here I have assumed the output softmax parameters are shared by input embedding layer parameters. This is assuming multi query attention also, so we achieve reduction of few parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters with MQA: 3212749824\n",
      "Number of parameters without MQA: 3565071360\n"
     ]
    }
   ],
   "source": [
    "## Code Verification\n",
    "\n",
    "config_llama3_2_3B = json.load(open('configs/llama-3.2-3B.json'))\n",
    "\n",
    "num_params_with_mqa = calculate_parameters(config_llama3_2_3B, use_mqa=True)\n",
    "num_params_without_mqa = calculate_parameters(config_llama3_2_3B, use_mqa=False)\n",
    "print(f\"Number of parameters with MQA: {num_params_with_mqa}\")\n",
    "print(f\"Number of parameters without MQA: {num_params_without_mqa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16777216"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096*4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA3.1-8B\n",
    "\n",
    "**Config:**\n",
    "\n",
    "- **hidden_size**: 4,096\n",
    "- **intermediate_size**: 14,336\n",
    "- **mlp_bias**: false\n",
    "- **num_attention_heads**: 32\n",
    "- **num_hidden_layers**: 32\n",
    "- **num_key_value_heads**: 8\n",
    "- **vocab_size**: 128,256\n",
    "- **head_dim**: hidden_size // num_attention_heads = 4,096 // 32 = 128\n",
    "- **tie_word_embeddings** : False\n",
    "\n",
    "### **1. Embedding Layer**\n",
    "\n",
    "- **Token Embeddings**: Maps `vocab_size` tokens to `hidden_size` embeddings.\n",
    "  - **Parameters**: `vocab_size × hidden_size`\n",
    "  - **Calculation**: `128,256 × 4,096 = 525,336,576`\n",
    "\n",
    "### **2. Transformer Layers**\n",
    "\n",
    "There are `num_hidden_layers = 32` Transformer layers. Each layer consists of:\n",
    "\n",
    "#### **a. Layer Normalization (RMSNorm)**\n",
    "\n",
    "- **Parameters per layer**: `2 × hidden_size` (since there are two RMSNorms per layer)\n",
    "  - **Calculation**: `2 × 4,096 = 8,192`\n",
    "\n",
    "#### **b. Self-Attention Mechanism**\n",
    "\n",
    "- **Query Projection (W_q)**:\n",
    "  - **Parameters**: `hidden_size × hidden_size`\n",
    "  - **Calculation**: `4,096 × 4,096 = 16,777,216`\n",
    "\n",
    "- **Key Projection (W_k)**:\n",
    "  - **Parameters**: `hidden_size × (num_key_value_heads × head_dim)`\n",
    "  - **Calculation**: `4,096 × (8 × 128) = 4,096 × 1,024 = 4,194,304`\n",
    "\n",
    "- **Value Projection (W_v)**:\n",
    "  - **Parameters**: Same as Key Projection\n",
    "  - **Calculation**: `4,194,304`\n",
    "\n",
    "- **Output Projection (W_o)**:\n",
    "  - **Parameters**: `(num_attention_heads × head_dim) × hidden_size`\n",
    "  - **Calculation**: `(32 × 128) × 4,096 = 4,096 × 4,096 = 16,777,216`\n",
    "\n",
    "- **Total Attention Parameters per layer**:\n",
    "  - **Calculation**: `16,777,216 (W_q) + 4,194,304 (W_k) + 4,194,304 (W_v) + 16,777,216 (W_o) = 41,943,040`\n",
    "\n",
    "#### **c. Feed-Forward Network (MLP)**\n",
    "\n",
    "- **Gate Proj**:\n",
    "  - **Parameters**: `hidden_size × intermediate_size`\n",
    "  - **Calculation**: `4,096 × 14,336 = 58,720,256`\n",
    "\n",
    "- **Up Proj**:\n",
    "  - **Parameters**: `hidden_size × intermediate_size`\n",
    "  - **Calculation**: `4,096 × 14,336 = 58,720,256`\n",
    "\n",
    "- **Down Proj**:\n",
    "  - **Parameters**: `intermediate_size × hidden_size`\n",
    "  - **Calculation**: `14,336 × 4,096 = 58,720,256`\n",
    "\n",
    "- **Total MLP Parameters per layer**:\n",
    "  - **Calculation**: `58,720,256 (gate_proj) + 58,720,256 (up_proj) + 58,720,256 (down_proj) = 176,160,768`\n",
    "\n",
    "#### **d. Total Parameters per Transformer Layer**\n",
    "\n",
    "- **Sum of all components per layer**:\n",
    "  - **Calculation**: `8,192 (LayerNorm) + 41,943,040 (Attention) + 176,160,768 (MLP) = 218,112,000`\n",
    "\n",
    "#### **e. Total Parameters for All Transformer Layers**\n",
    "\n",
    "- **Total for 32 layers**:\n",
    "  - **Calculation**: `218,112,000 × 32 = 6,979,584,000`\n",
    "\n",
    "### **3. Final Layer Normalization**\n",
    "\n",
    "- **Parameters**: `hidden_size`\n",
    "  - **Calculation**: `4,096`\n",
    "\n",
    "### **4. Total Model Parameters**\n",
    "\n",
    "- **Sum of Embedding Layer, Transformer Layers, and Final LayerNorm**:\n",
    "  - **Calculation**: `2 * 525,336,576 (Embeddings) + 6,979,584,000 (Transformer Layers) + 4,096 (Final LayerNorm)  = 8,030,261,248`\n",
    "\n",
    "**Answer**: The model has a total of 8,030,261,248 parameters. Here embedding tying is False, therefore input embedding weights are also accounted for the weight calculation, thats why there is 2 * 525,336,576\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters with MQA: 8030261248\n",
      "Number of parameters without MQA: 8835567616\n"
     ]
    }
   ],
   "source": [
    "## Code Verification\n",
    "\n",
    "config_llama3_1_8B = json.load(open('configs/llama-3.1-8B.json'))\n",
    "\n",
    "num_params_with_mqa = calculate_parameters(config_llama3_1_8B, use_mqa=True)\n",
    "num_params_without_mqa = calculate_parameters(config_llama3_1_8B, use_mqa=False)\n",
    "print(f\"Number of parameters with MQA: {num_params_with_mqa}\")\n",
    "print(f\"Number of parameters without MQA: {num_params_without_mqa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA 3.1 70B\n",
    "\n",
    "**Config:**\n",
    "\n",
    "- **hidden_size**: 8,192\n",
    "- **intermediate_size**: 28,672\n",
    "- **mlp_bias**: false\n",
    "- **num_attention_heads**: 64\n",
    "- **num_hidden_layers**: 80\n",
    "- **num_key_value_heads**: 8\n",
    "- **vocab_size**: 128,256\n",
    "- **head_dim**: hidden_size // num_attention_heads = 4,096 // 32 = 128\n",
    "- **tie_word_embeddings** : False\n",
    "\n",
    "### 1. Embedding Layer\n",
    "\n",
    "- **Token Embeddings**:\n",
    "  - Parameters: `vocab_size × hidden_size = 128,256 × 8,192 = 1,050,673,152`\n",
    "- **Output Embeddings** (since `tie_word_embeddings` is False):\n",
    "  - Parameters: Same as token embeddings\n",
    "  - Parameters: `1,050,673,152`\n",
    "- **Total Embedding Parameters**:\n",
    "  - Calculation: `1,050,673,152 (Token Embeddings) + 1,050,673,152 (Output Embeddings) = 2,101,346,304`\n",
    "\n",
    "### 2. Transformer Layers\n",
    "\n",
    "There are `num_hidden_layers = 80` Transformer layers. Each layer consists of:\n",
    "\n",
    "#### a. Layer Normalization (RMSNorm)\n",
    "\n",
    "- **Parameters per layer**:\n",
    "  - Calculation: `2 × hidden_size = 2 × 8,192 = 16,384`\n",
    "\n",
    "#### b. Self-Attention Mechanism\n",
    "\n",
    "- **Query Projection (W_q)**:\n",
    "  - Parameters: `hidden_size × hidden_size`\n",
    "  - Calculation: `8,192 × 8,192 = 67,108,864`\n",
    "- **Key Projection (W_k)**:\n",
    "  - Parameters: `hidden_size × (num_key_value_heads × head_dim)`\n",
    "  - Calculation: `8,192 × (8 × 128) = 8,192 × 1,024 = 8,388,608`\n",
    "- **Value Projection (W_v)**:\n",
    "  - Parameters: Same as Key Projection\n",
    "  - Calculation: `8,388,608`\n",
    "- **Output Projection (W_o)**:\n",
    "  - Parameters: `(num_attention_heads × head_dim) × hidden_size`\n",
    "  - Calculation: `(64 × 128) × 8,192 = 8,192 × 8,192 = 67,108,864`\n",
    "- **Total Attention Parameters per layer**:\n",
    "  - Calculation: `67,108,864 (W_q) + 8,388,608 (W_k) + 8,388,608 (W_v) + 67,108,864 (W_o) = 150,994,944`\n",
    "\n",
    "#### c. Feed-Forward Network (MLP)\n",
    "\n",
    "- **Gate Proj**:\n",
    "  - Parameters: `hidden_size × intermediate_size`\n",
    "  - Calculation: `8,192 × 28,672 = 234,881,024`\n",
    "- **Up Proj**:\n",
    "  - Parameters: `hidden_size × intermediate_size`\n",
    "  - Calculation: `8,192 × 28,672 = 234,881,024`\n",
    "- **Down Proj**:\n",
    "  - Parameters: `intermediate_size × hidden_size`\n",
    "  - Calculation: `28,672 × 8,192 = 234,881,024`\n",
    "- **Total MLP Parameters per layer**:\n",
    "  - Calculation: `234,881,024 (Gate Proj) + 234,881,024 (Up Proj) + 234,881,024 (Down Proj) = 704,643,072`\n",
    "\n",
    "#### d. Total Parameters per Transformer Layer\n",
    "\n",
    "- **Sum of all components per layer**:\n",
    "  - Calculation: `16,384 (LayerNorm) + 150,994,944 (Attention) + 704,643,072 (MLP) = 855,654,400`\n",
    "\n",
    "#### e. Total Parameters for All Transformer Layers\n",
    "\n",
    "- **Total for 80 layers**:\n",
    "  - Calculation: `855,654,400 × 80 = 68,452,352,000`\n",
    "\n",
    "### 3. Final Layer Normalization\n",
    "\n",
    "- **Parameters**:\n",
    "  - Calculation: `hidden_size = 8,192`\n",
    "\n",
    "### 4. Total Model Parameters\n",
    "\n",
    "- **Sum of Embedding Layer, Transformer Layers, and Final LayerNorm**:\n",
    "  - Calculation: `2,101,346,304 (Embeddings) + 68,452,352,000 (Transformer Layers) + 8,192 (Final LayerNorm) = 70,553,706,496`\n",
    "\n",
    "**Answer**: The model has a total of **70,553,706,496** parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters with MQA: 70553706496\n",
      "Number of parameters without MQA: 79948947456\n"
     ]
    }
   ],
   "source": [
    "## Code Verification\n",
    "\n",
    "config_llama3_1_70B = json.load(open('configs/llama-3.1-70B.json'))\n",
    "\n",
    "num_params_with_mqa = calculate_parameters(config_llama3_1_70B, use_mqa=True)\n",
    "num_params_without_mqa = calculate_parameters(config_llama3_1_70B, use_mqa=False)\n",
    "print(f\"Number of parameters with MQA: {num_params_with_mqa}\")\n",
    "print(f\"Number of parameters without MQA: {num_params_without_mqa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['LlamaForCausalLM'],\n",
       " 'attention_bias': False,\n",
       " 'attention_dropout': 0.0,\n",
       " 'bos_token_id': 128000,\n",
       " 'eos_token_id': 128001,\n",
       " 'hidden_act': 'silu',\n",
       " 'hidden_size': 16384,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 53248,\n",
       " 'max_position_embeddings': 131072,\n",
       " 'mlp_bias': False,\n",
       " 'model_type': 'llama',\n",
       " 'num_attention_heads': 128,\n",
       " 'num_hidden_layers': 126,\n",
       " 'num_key_value_heads': 8,\n",
       " 'pretraining_tp': 1,\n",
       " 'rms_norm_eps': 1e-05,\n",
       " 'rope_scaling': {'factor': 8.0,\n",
       "  'low_freq_factor': 1.0,\n",
       "  'high_freq_factor': 4.0,\n",
       "  'original_max_position_embeddings': 8192,\n",
       "  'rope_type': 'llama3'},\n",
       " 'rope_theta': 500000.0,\n",
       " 'tie_word_embeddings': False,\n",
       " 'torch_dtype': 'bfloat16',\n",
       " 'transformers_version': '4.42.3',\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 128256}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_llama3_1_405B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA 3.1 405B\n",
    "\n",
    "**Config:**\n",
    "\n",
    "- **hidden_size**: 16,384\n",
    "- **intermediate_size**: 53,248\n",
    "- **mlp_bias**: false\n",
    "- **num_attention_heads**: 128\n",
    "- **num_hidden_layers**: 126\n",
    "- **num_key_value_heads**: 8\n",
    "- **vocab_size**: 128,256\n",
    "- **head_dim**: hidden_size // num_attention_heads = 4,096 // 32 = 128\n",
    "- **tie_word_embeddings** : False\n",
    "\n",
    "### 1. Embedding Layer\n",
    "\n",
    "- **Token Embeddings**:\n",
    "  - Parameters: `vocab_size × hidden_size = 128,256 × 16,384 = 2,101,346,304`\n",
    "- **Output Embeddings** (since `tie_word_embeddings` is False):\n",
    "  - Parameters: `2,101,346,304`\n",
    "- **Total Embedding Parameters**:\n",
    "  - Calculation: `2,101,346,304 + 2,101,346,304 = 4,202,692,608`\n",
    "\n",
    "### 2. Transformer Layers\n",
    "\n",
    "There are `num_hidden_layers = 126` Transformer layers. Each layer consists of:\n",
    "\n",
    "#### a. Layer Normalization (RMSNorm)\n",
    "\n",
    "- **Parameters per layer**:\n",
    "  - Calculation: `2 × hidden_size = 2 × 16,384 = 32,768`\n",
    "\n",
    "#### b. Self-Attention Mechanism\n",
    "\n",
    "- **Query Projection (W_q)**:\n",
    "  - Parameters: `hidden_size × hidden_size = 16,384 × 16,384 = 268,435,456`\n",
    "- **Key Projection (W_k)**:\n",
    "  - Parameters: `hidden_size × (num_key_value_heads × head_dim) = 16,384 × (8 × 128) = 16,384 × 1,024 = 16,777,216`\n",
    "- **Value Projection (W_v)**:\n",
    "  - Parameters: `16,777,216` (same as W_k)\n",
    "- **Output Projection (W_o)**:\n",
    "  - Parameters: `(num_attention_heads × head_dim) × hidden_size = (128 × 128) × 16,384 = 16,384 × 16,384 = 268,435,456`\n",
    "- **Total Attention Parameters per layer**:\n",
    "  - Calculation: `268,435,456 (W_q) + 16,777,216 (W_k) + 16,777,216 (W_v) + 268,435,456 (W_o) = 570,425,344`\n",
    "\n",
    "#### c. Feed-Forward Network (MLP)\n",
    "\n",
    "- **Gate Proj**:\n",
    "  - Parameters: `hidden_size × intermediate_size = 16,384 × 53,248 = 872,415,232`\n",
    "- **Up Proj**:\n",
    "  - Parameters: `16,384 × 53,248 = 872,415,232`\n",
    "- **Down Proj**:\n",
    "  - Parameters: `intermediate_size × hidden_size = 53,248 × 16,384 = 872,415,232`\n",
    "- **Total MLP Parameters per layer**:\n",
    "  - Calculation: `872,415,232 + 872,415,232 + 872,415,232 = 2,617,245,696`\n",
    "\n",
    "#### d. Total Parameters per Transformer Layer\n",
    "\n",
    "- **Sum of all components per layer**:\n",
    "  - Calculation: `32,768 (LayerNorm) + 570,425,344 (Attention) + 2,617,245,696 (MLP) = 3,187,703,808`\n",
    "\n",
    "#### e. Total Parameters for All Transformer Layers\n",
    "\n",
    "- **Total for 126 layers**:\n",
    "  - Calculation: `3,187,703,808 × 126 = 401,650,679,808`\n",
    "\n",
    "### 3. Final Layer Normalization\n",
    "\n",
    "- **Parameters**:\n",
    "  - Calculation: `hidden_size = 16,384`\n",
    "\n",
    "### 4. Total Model Parameters\n",
    "\n",
    "- **Sum of Embedding Layer, Transformer Layers, and Final LayerNorm**:\n",
    "  - Calculation: `4,202,692,608 (Embeddings) + 401,650,679,808 (Transformer Layers) + 16,384 (Final LayerNorm) = 405,853,388,800`\n",
    "\n",
    "**Answer**: The model has a total of **405,853,388,800** parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters with MQA: 405853388800\n",
      "Number of parameters without MQA: 469271265280\n"
     ]
    }
   ],
   "source": [
    "## Code Verification\n",
    "\n",
    "config_llama3_1_405B = json.load(open('configs/llama-3.1-405B.json'))\n",
    "\n",
    "num_params_with_mqa = calculate_parameters(config_llama3_1_405B, use_mqa=True)\n",
    "num_params_without_mqa = calculate_parameters(config_llama3_1_405B, use_mqa=False)\n",
    "print(f\"Number of parameters with MQA: {num_params_with_mqa}\")\n",
    "print(f\"Number of parameters without MQA: {num_params_without_mqa}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
