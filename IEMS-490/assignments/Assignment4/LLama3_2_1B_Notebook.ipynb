{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT9e_GnTR5wW",
        "outputId": "f0cdb938-ab2d-4e9c-c37a-140c8fa4b7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = open('hf_token.txt','r').read()\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Install Relevant Packages\n",
        "#!pip install -U bitsandbytes\n",
        "#!pip install -U transformers accelerate torch"
      ],
      "metadata": {
        "id": "lZFn7JXcUarh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the BitsAndBytesConfig for 8-bit quantization to fit into colab's GPU memory\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if not tokenizer.pad_token:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             device_map=\"auto\")"
      ],
      "metadata": {
        "id": "4MyQ-pQFSFHR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xcl0lBCTPmJ",
        "outputId": "ac5bc580-5b6f-436f-97b6-05b8c2958abb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, max_length=100):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  # Tokenize the input text and set padding and attention_mask\n",
        "  input_data = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "  # Get input IDs and attention mask\n",
        "  input_ids = input_data['input_ids'].to(device)\n",
        "  attention_mask = input_data['attention_mask'].to(device)\n",
        "  # Generate the model's response\n",
        "  with torch.no_grad():\n",
        "      output_ids = model.generate(input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  max_length=max_length)\n",
        "\n",
        "  # Decode the output\n",
        "  output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "  print(\"\\nGenerated Text.....\\n\")\n",
        "  print(output_text)"
      ],
      "metadata": {
        "id": "T3V5oWLFnaZe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\n",
        "    \"Explain what is the difference between supervised and unsupervised learning\",\n",
        "    max_length=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ieZSSIWOx_",
        "outputId": "f7d5c02d-ec2a-42fd-a0c6-12fec214bb60"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text.....\n",
            "\n",
            "Explain what is the difference between supervised and unsupervised learning\n",
            "Unsupervised learning is the process of discovering patterns in data without any labels or target values. In supervised learning, a training dataset is provided with the target values, and the algorithm learns to predict the target values based on the input features. Unsupervised learning is a subset of supervised learning, where the algorithm learns to discover patterns in the data without any labels or target values.\n",
            "In supervised learning, a training dataset is provided with the target values, and the algorithm learns to predict the target values based on the input features. Unsupervised learning is a subset of supervised learning, where the algorithm learns to discover patterns in the data without any labels or target values. In supervised learning, a training dataset is provided with the target values, and the algorithm learns to predict the target values based on the input features. Unsupervised learning is a subset of supervised learning, where the algorithm learns to discover patterns in the data without any\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\n",
        "    \"What is cross entropy?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odZ4DTaOnYeq",
        "outputId": "6b77757e-7e47-4ac1-b49c-e9208743a81d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text.....\n",
            "\n",
            "What is cross entropy? How is it used in machine learning? In this article, we’ll explain what cross entropy is, how it’s calculated, and how it’s used in machine learning.\n",
            "What is cross entropy?\n",
            "Cross entropy is a measure of the difference between the probabilities of two events. It is used to compare the probability of a given event occurring with the probability of a different event occurring. It is a type of error rate and is used to measure the quality of a classifier.\n",
            "How\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=100,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "response = pipe(\"What is binary cross entropy loss?\")\n",
        "for _ in response:\n",
        "  print()\n",
        "  print(_.get('generated_text'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKIiRO56r843",
        "outputId": "7ad90343-952f-4297-b302-4d8302aee0a3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What is binary cross entropy loss? – Tutorialspoint\n",
            "The cross entropy loss is a measure of the difference between the predicted and the actual label. The cross entropy loss is used in machine learning to calculate the loss function for a given model. In this article, we will learn what is binary cross entropy loss and how to calculate it. The cross entropy loss is a measure of the difference between the predicted and the actual label. The cross entropy loss is used in machine learning to calculate the loss\n"
          ]
        }
      ]
    }
  ]
}