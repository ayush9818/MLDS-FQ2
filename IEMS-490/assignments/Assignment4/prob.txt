Look at the model architectures of Llama 3.2 (3B and 1B) and Llama 3.1 (405B, 70B, and 8B). Provide a detailed calculation of their parameter counts (e.g., where was the 1B parameters assigned - how many parameters for FFNs and how many parameters for attention and so on). Use your university email to register.

Download the 1B model and get it running. (This is an open-ended question. You are encouraged to use any inference framework. A reasonable laptop should be able to host it but you can also use colab.) Submit a Python notebook.
